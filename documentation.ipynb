{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sources* :\n",
    "[How to Develop Word Embeddings in Python with Gensim, *machinelearningmastery.com*](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n",
    "\n",
    "[How to Develop Word Embeddings in Python with Gensim, *machinelearningmastery.com*](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n",
    "\n",
    "[Understanding LSTM Networks, *colah.github.io*](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "[CNN Long Short-Term Memory Networks, *machinelearningmastery.com*](https://machinelearningmastery.com/cnn-long-short-term-memory-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning a text example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split tokens on white space.\n",
    "- Remove all punctuation from words.\n",
    "- Remove all words that are not purely comprised of alphabetical characters.\n",
    "- Remove all words that are known stop words.\n",
    "- Remove all words that have a length <= 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.387786Z",
     "start_time": "2019-07-27T09:58:07.783597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/macbook/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbook/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.591155Z",
     "start_time": "2019-07-27T09:58:13.392890Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'metamorphosis_clean.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5d4aaf502adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'metamorphosis_clean.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'metamorphosis_clean.txt'"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.609321Z",
     "start_time": "2019-07-27T09:58:07.789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.611906Z",
     "start_time": "2019-07-27T09:58:07.794Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.614357Z",
     "start_time": "2019-07-27T09:58:07.799Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.616830Z",
     "start_time": "2019-07-27T09:58:07.803Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.619120Z",
     "start_time": "2019-07-27T09:58:07.808Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in\n",
    "         words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.621937Z",
     "start_time": "2019-07-27T09:58:07.812Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stemming of words : reduces each word to its root or base\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split tokens on white space.\n",
    "- Remove all punctuation from words.\n",
    "- Remove all words that are not purely comprised of alphabetical characters.\n",
    "- Remove all words that are known stop words.\n",
    "- Remove all words that have a length <= 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.628307Z",
     "start_time": "2019-07-27T09:58:07.817Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.630904Z",
     "start_time": "2019-07-27T09:58:07.821Z"
    }
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.633294Z",
     "start_time": "2019-07-27T09:58:07.825Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'review_polarity/txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to define a vocabulary of known words when using a bag-of-words or embedding model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their counts that allow us to easily update and query.\n",
    "\n",
    "$\\implies$ register the occurrence of each word (after cleaning = removing punctuation and numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.635964Z",
     "start_time": "2019-07-27T09:58:07.830Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.643333Z",
     "start_time": "2019-07-27T09:58:07.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.646577Z",
     "start_time": "2019-07-27T09:58:07.839Z"
    }
   },
   "outputs": [],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('review_polarity/txt_sentoken/neg', vocab, True)\n",
    "process_docs('review_polarity/txt_sentoken/pos', vocab, True)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.649124Z",
     "start_time": "2019-07-27T09:58:07.843Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the top words in the vocab (N.B. no more stopwords)\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.652226Z",
     "start_time": "2019-07-27T09:58:07.847Z"
    }
   },
   "outputs": [],
   "source": [
    "# keep tokens with a min occurrence (here 2)\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.654617Z",
     "start_time": "2019-07-27T09:58:07.851Z"
    }
   },
   "outputs": [],
   "source": [
    "# save list  (= tokens) to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "    \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exemple* : take the sentence **So hungry, need food** and break it down into four arbitrary symbols: **so** represented as K45, **hungry** as J83, **need** as Q67, and **food** as P21, all of which can then be processed by the computer. Each unique word is represented by a different symbol; however, the downside is that there is no apparent relationship between the symbols designated to **hungry** and **food**. This hinders the NLP model from using what it learned about hungry and applying it to food, which are semantically related. Vector Space Models (VSM) help address this issue by embedding the words in a vector space where similarly defined words are mapped near each other. This space is called a Word Embedding.\n",
    "![](images/word_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are word embeddings for text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are word embeddings ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **word embedding** is a way of *representing text* where *each word* in the vocabulary is represented by a *real valued vector in a high-dimensional space*. The vectors are learned in such a way that *words that have similar meanings will have similar representation in the vector space* (close in the vector space).<br><br>\n",
    "$\\implies$ word embeddings = a class of techniques where individual words are represented as real-valued vectors in a predefined vector space $\\to$ each word is mapped to one vector and the vector values are learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words embeddings algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embedding methods** learn a *real-valued vector representation* for a *predefined fixed sized vocabulary* from a corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIRES THE DOCUMENT TO BE CLEANED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Embedding Layer** is a *word embedding* that is learned jointly with a *neural network model* on a specific natural language processing task.\n",
    "- The size of the vector space is specified as part of the model\n",
    "- The embedding layer is used on the front end of a neural network and is fit in a supervised way using the Backpropagation algorithm\n",
    "- If a multilayer Perceptron model is used, then the word vectors are concatenated before being fed as input to the model\n",
    "- If a recurrent neural network is used, then each word may be taken as one input in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2vec** (founded by a Google team) is one of the most popular *models* used to *create word embeddings*.\n",
    "\n",
    "2 methods :\n",
    "- *Continuous Bag-of-Words model (CBOW)* = the less popular of the two models, uses source words to predict the target words<br> $\\implies$ \"I want to learn Python\" uses \"I want to learn\" to predict \"Python\"<br><br>\n",
    "- *Skip-Gram Model* = uses target words to predict the source, or context<br> $\\implies$ \"The quick brown fox jumped over the lazy dog\" $\\to$ breaks the sentence in pairs (context = 2 words surrounding, target) $\\implies$ ([the, brown],quick), ([quick,fox],brown), ([brown,jumped],fox),... $\\to$ pairs reduced to (input = word, output = 1/2 of the 2 words surrounding) (input = word, output = 1/2 of the 2 words surrounding)<br>\n",
    "![](images/Word2Vec-Training-Models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.657580Z",
     "start_time": "2019-07-27T09:58:07.863Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.660138Z",
     "start_time": "2019-07-27T09:58:07.868Z"
    }
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "print (vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.662675Z",
     "start_time": "2019-07-27T09:58:07.873Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = vocab.split()\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.665168Z",
     "start_time": "2019-07-27T09:58:07.877Z"
    }
   },
   "outputs": [],
   "source": [
    "print (set(vocab[:10]))\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load all of the training data movie reviews $\\implies$ update process-doc() $\\to$ load the documents (pos, neg) + clean them + return them as a list of strings (1 document per string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.667786Z",
     "start_time": "2019-07-27T09:58:07.883Z"
    }
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('review_polarity/txt_sentoken/pos', vocab, True)\n",
    "print(positive_docs[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.673182Z",
     "start_time": "2019-07-27T09:58:07.887Z"
    }
   },
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "negative_docs = process_docs('review_polarity/txt_sentoken/neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    "print (negative_docs[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode documents as sequences of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T14:16:07.392657Z",
     "start_time": "2019-06-25T14:16:07.383541Z"
    }
   },
   "source": [
    "- The Keras Embedding layer requires integer inputs $\\implies$ use the **Tokenizer class**\n",
    "- The Embedding requires the specification of the vocabulary size + the size of the real-valued vector space + the maximum length of input documents\n",
    "- 1 token = 1 vector\n",
    "- Vectors are at first random and become meaningful\n",
    "- Ensure that all documents have the same length for Keras efficient computation\n",
    "- Create class labels for the neural network\n",
    "- Encode and pad the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.676063Z",
     "start_time": "2019-07-27T09:58:07.893Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the tokenizer = instance of class\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents = develops a consistent mapping from words in the vocabulary to unique integers\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\implies$ the mapping of words to integers is prepared\n",
    "    $\\implies$ use it to encode the reviews in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.678699Z",
     "start_time": "2019-07-27T09:58:07.898Z"
    }
   },
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:20:55.421430Z",
     "start_time": "2019-06-25T15:20:55.416273Z"
    }
   },
   "source": [
    "We also need to ensure that all documents have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.681736Z",
     "start_time": "2019-07-27T09:58:07.903Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post') # adds 0 to reach the lenght of max_lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the class labels for the training dataset (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.684329Z",
     "start_time": "2019-07-27T09:58:07.908Z"
    }
   },
   "outputs": [],
   "source": [
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "print (ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then encode and pad the test dataset, needed later to evaluate the model after we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.689369Z",
     "start_time": "2019-07-27T09:58:07.913Z"
    }
   },
   "outputs": [],
   "source": [
    "# load all test reviews\n",
    "positive_docs = process_docs('review_polarity/txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('review_polarity/txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.692051Z",
     "start_time": "2019-07-27T09:58:07.916Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network model and Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An Embedding** is *mapping a discrete variable* into a *vector of continuous numbers*<br>\n",
    "$\\implies$  first randomly initializes the embedding vector and then uses network optimizer to update it similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“deep learning is very deep” $\\to$ 1 2 3 4 1\n",
    "The embedding matrix gets created next : we decide how many ‘latent factors’ are assigned to each index = how long we want the vector to be (mostly 32 or 50).\n",
    "Instead of ending up with huge one-hot encoded vectors we can use an embedding matrix to keep the size of each vector much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding requires the specification of the vocabulary size (= total number of words + 1 (for unknown words)) + the size of the real-valued vector space (here a 100-d) + the maximum length of input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set consists only of two phrases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope to see you soon<br>\n",
    "Nice to see you soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T15:53:18.029065Z",
     "start_time": "2019-07-05T15:53:18.024205Z"
    }
   },
   "source": [
    "We encode it :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope to see you soon $\\to$ [0, 1, 2, 3, 4]<br>\n",
    "Nice to see you again $\\to$ [5, 1, 2, 3, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T15:52:22.896827Z",
     "start_time": "2019-07-05T15:52:22.793969Z"
    }
   },
   "source": [
    "We want to train a network whose first layer is an embeding layer. In this case, we should initialize it as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.696399Z",
     "start_time": "2019-07-27T09:58:07.927Z"
    }
   },
   "outputs": [],
   "source": [
    "Embedding(7, 2, input_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first argument (7) is the number of distinct words in the training set\n",
    "- The second argument (2) indicates the size of the embedding vectors\n",
    "- The input_length argument determines the size of each input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network has been trained, we can get the weights of the embedding layer, which in this case will be of size (7, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table used to map integers to embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these embeddings the sentence \"Nice to see you agaign\" will be reprensented as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.699401Z",
     "start_time": "2019-07-27T09:58:07.934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[[0.7, 1.7], [0.1, 4.2], [1.0, 3.1], [0.3, 2.1], [4.1, 2.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.702060Z",
     "start_time": "2019-07-27T09:58:07.939Z"
    }
   },
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value) N.B. : all the words of the texts will be classified in two categories : the vocabulary and the unknown words\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.704086Z",
     "start_time": "2019-07-27T09:58:07.944Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Convolutional Neural Network** = a Deep Learning algorithm which takes an input image *assign importance* (weights + biases) to various *aspects of the image*, which it can differentiate.<br><br>\n",
    "$\\to$ The CNN is an **alternance of convolutions and poolings** $\\Longleftrightarrow$1 layer = 1 convolution layer + 1 pooling layer.<br><br>\n",
    "The pre-processing requiries a ConvNet which captures the Spatial and Temporal dependencies thanks to filters, its role is to reduce the image in an easier form to process, without losing features which are critical for getting a good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_architecture.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A convolution :** here with a Stride Length = 1, weights are fitting during the convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/kernel.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding :** applied with the convolution, the pixels of the corners are less counted than those in the middle = inequality weights $\\implies$ loss of data $\\implies$ we give additional pixels at the boundary of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/padding.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A pooling layer :** we only take the maximum value/the average inside the box on the left case, usually a Maxpooling layer which is a Noise Suppressant.<br> $\\implies$ reduces the spatial size of the Convolved Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pooling_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flattening :** the final output of the layer(s) is converting the data into a 1-dimensional array (column vector).\n",
    "\n",
    "**Fully-connected layer or Dense layer :** learns the non-linear combinations of the high-level features. Always has an input and output layers and the layers in between are called the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/flattening_fully_connected_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Unit** = Neuron consisting of : \n",
    "    - $a_j(t)$ : the activation (active or unactive) = the neuron's state.<br>\n",
    "    \n",
    "    - $\\theta_j$ : a threshold which is fixed unless changed by a learning function, the neuron $j$ is activated if the input overpass the threshold.<br>\n",
    "    \n",
    "    - $a_j(t+1) = f(a_j(t),p_j(t),\\theta_j)$ : an (predefined) activation function which actives or desactivates the neuron.<br>\n",
    "    \n",
    "    - $o_j(t) = f_{out}(a_j(t))$ : the output function which works if activated.\n",
    "    \n",
    "\n",
    "- A $j$ neuron receives an input $p_j$ from predecessor neurons : $p_j(t) = \\sum o_j(t)w_{ij}+ w_{0j}$ where $w_{0j}$ is a bias = can be add for learning the threashold $\\theta_j$.\n",
    "\n",
    "\n",
    "- **Hyper-parameters** : set manually (number of filters, of layers, ...).\n",
    "\n",
    "\n",
    "- **Cost function** : $C$, considering the whole network as a function looks for the optimal solution (for a given task) i.e. $C(f*) \\leq C(f) \\forall f \\in F$ which means the network modifies its parameters (weights and biases) until it reaches the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.707129Z",
     "start_time": "2019-07-27T09:58:07.957Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the network on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a **binary cross entropy loss function** because the problem we are learning is a binary classification problem.<br><br> $\\implies$ Adam implementation of stochastic gradient descent (= designed specifically for training deep neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.710046Z",
     "start_time": "2019-07-27T09:58:07.962Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is evaluated on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.712417Z",
     "start_time": "2019-07-27T09:58:07.967Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T15:43:15.080344Z",
     "start_time": "2019-06-28T15:43:15.032249Z"
    }
   },
   "source": [
    "### Train with word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.714927Z",
     "start_time": "2019-07-27T09:58:07.972Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word2vec algorithm** = approach to *learning a word embedding* from a text corpus in a *standalone way*.<br><br>\n",
    "$\\implies$ can produce high-quality word embeddings very efficiently (space and time complexity)\n",
    "- The word2vec algorithm processes documents sentence by sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to develop word embeddings in Python with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** = open source Python library for natural language processing $\\implies$ topic modelling for humans\n",
    "- Suite of Natural Language Processing tools for topic modeling\n",
    "- Tools for loading pre-trained word embeddings in a few formats and for making use and querying a loaded embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the many *parameters* : \n",
    "- size (default 100) : the number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word)\n",
    "- window (default 5) : the maximum distance between a target word and words around the target word\n",
    "- min_count (default 5) : the minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored\n",
    "- workers (default 3) : the number of threads to use while training $\\to$ higher if get many cores (8) \n",
    "- sg (default 0 or CBOW) : the training algorithm, either CBOW (0) or skip gram (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the documents = the same data cleaning steps from the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.717695Z",
     "start_time": "2019-07-27T09:58:07.980Z"
    }
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.721077Z",
     "start_time": "2019-07-27T09:58:07.984Z"
    }
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens = clean line by line and return cleaned lines\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.723959Z",
     "start_time": "2019-07-27T09:58:07.988Z"
    }
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory = load and clean all of the documents in a folder and return a list of all document lines\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # add lines to list\n",
    "        lines += doc_lines\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.726540Z",
     "start_time": "2019-07-27T09:58:07.993Z"
    }
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "positive_lines = process_docs('review_polarity/txt_sentoken/pos', vocab, True)\n",
    "negative_lines = process_docs('review_polarity/txt_sentoken/neg', vocab, True)\n",
    "sentences = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create the model : clean sentences  + size of the embedding vector space (here 100) + number of neighboring words to look at (here 5) + number of threads to use when fitting the model (here 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.729047Z",
     "start_time": "2019-07-27T09:58:07.998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "print (model)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.731443Z",
     "start_time": "2019-07-27T09:58:08.002Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.732953Z",
     "start_time": "2019-07-27T09:58:08.007Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use classical projection methods to reduce the high-dimensional word vectors to two-dimensional plots and plot them on a graph.\n",
    "1. Retrieve all of the vectors from a trained model\n",
    "<br>$\\implies$ use **PCA** with **Pizo**\n",
    "2. Train a projection method on the vectors\n",
    "3. Use matplotlib to plot the projection as a scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use Pizo to plot the figure as Jupyter failed (= the kernel crashed). With this in mind we save text and X in files in order to access it from Pizo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.736493Z",
     "start_time": "2019-07-27T09:58:08.013Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = [sentence.split() for sentence in sentences]\n",
    "print([t[:5] for t in text[:10]])\n",
    "with open(\"text.txt\", \"wb\") as fp:   # save text in a file for Pyzo\n",
    "    pickle.dump(text, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.738750Z",
     "start_time": "2019-07-27T09:58:08.017Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "model_visu = Word2Vec(text, size=100, window=5, workers=8, min_count=100)\n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec_visu.txt'\n",
    "model_visu.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.740940Z",
     "start_time": "2019-07-27T09:58:08.021Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = model_visu[model_visu.wv.vocab]\n",
    "# np.save('X.npy',X) # save with numpy as an arrayhh\n",
    "#with open('X_list.pkl','wb') as f: # save with pickle as a list\n",
    "    #pickle.dump(list(X), f)\n",
    "with open ('X.pkl','wb') as f : # save with pickle as an array\n",
    "        pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.743091Z",
     "start_time": "2019-07-27T09:58:08.026Z"
    }
   },
   "outputs": [],
   "source": [
    "# in Pizo\n",
    "#def visualize_we (model) :\n",
    "#    pca = PCA(n_components=2)\n",
    "#    X = np.load('X.npy')\n",
    "#    with open(\"text.txt\", \"rb\") as fp:   \n",
    "#    text = pickle.load(fp)\n",
    "#    model_visu = Word2Vec(text, size=100, window=5, workers=8, min_count=1)\n",
    "#    X = model[model.wv.vocab]\n",
    "#    pca = PCA(n_components=2) # 2-dimensional PCA\n",
    "#    result = pca.fit_transform(X)\n",
    "#    # create a scatter plot of the projection\n",
    "#    pyplot.scatter(result[:, 0], result[:, 1])\n",
    "#    words = list(model.wv.vocab)\n",
    "#    print (words)\n",
    "#    for i, word in enumerate(words):\n",
    "#        pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "#    pyplot.show()\n",
    "\n",
    "#visualize_we(model_visu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.745581Z",
     "start_time": "2019-07-27T09:58:08.031Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import asarray\n",
    "from numpy import array\n",
    "from numpy import zeros\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a word vectors is complicated $\\implies$ use an existing pre-trained word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that the loaded embedding does not contain all of the words in our chosen vocabulary $\\implies$ skip words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.747963Z",
     "start_time": "2019-07-27T09:58:08.036Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns a directory of words mapped to the vectors in NumPy format\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line ( load the word embedding as a directory of words to vectors)\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.750917Z",
     "start_time": "2019-07-27T09:58:08.041Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add this layer to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T09:58:13.752852Z",
     "start_time": "2019-07-27T09:58:08.045Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model_pre_trained = Sequential()\n",
    "model_pre_trained.add(embedding_layer_pre_trained)\n",
    "model_pre_trained.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model_pre_trained.add(MaxPooling1D(pool_size=2))\n",
    "model_pre_trained.add(Flatten())\n",
    "model_pre_trained.add(Dense(1, activation='sigmoid'))\n",
    "print(model_pre_trained.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short Term Memory networks (LSTMs)** are a special kind of RNN, capable of *learning long-term dependencies*.<br><br>\n",
    "\n",
    "$\\to$ **RNN** are networks with *loops* allowing the information to persist, it can be seen as a set of copies of the same network where each passes a message to its successor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rnn_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTMs** have a chain-like structure with four neural network layers :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lstm_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lstm_architecture_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell state** = conveyor belt which carries information that can be add or removed from *gates*, it can be seen as the memory of the network which carries the information necessarily to make good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lstm_C_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gates** = a sigmoid neural net layer (outputs numbers between zero and one) and a pointwise multiplication operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lstm_gates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Forget gate layer** : decides what information will be throw away.<br>\n",
    "Takes as inputs $h_{t-1} = o_{t-1} o \\sigma_{t-1}$, where $o_{t-1}$ is the output gate's activation vector and $\\sigma_{t-1}$ is a sigmoid function, and $x(t)$ a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T10:20:30.702105Z",
     "start_time": "2019-07-07T10:20:30.367364Z"
    }
   },
   "source": [
    "![](images\\lstm_forget_gate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\implies$ outputs a number between 0 (completly get rid of it) and 1 (completly keep it) for each number of the cell state $C_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input gate** = a *sigmoid layer* $\\sigma$ : decides which values will be updated.<br>\n",
    "Then a *tanh layer* creates new candidate values $\\tilde{C}_t $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\lstm_inpu_gate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\implies$ we add $i_t$ * $\\tilde{C}_t $ to the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ouput layer** : decides what will be ouput, a sigmoid layer $\\sigma$ is running to choose the part of the cell state to ouput ($o_t$ gives a value we will apply to the cell state), then a tanh function is running to have values between -1 and 1 (normalization and having a no so linear result to extract the most important features). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lstm_ouput_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM** = efficient for data prediction which requiries to consider both close and far elements from a position\n",
    "- Uses *Backpropagation Through Time* (BPTT) for updating the weights = modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs\n",
    "- Each time step = one CNN model + sequence of LSTM models\n",
    "- At the backend the CNN layer(s) is wrapped in a **TimeDistributed layer** = apply a convolutional layer using TimeDistributed (applicable to a 1-d) along a time dimension in order to obtain a 2-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T16:31:32.953850Z",
     "start_time": "2019-07-04T16:31:32.946201Z"
    }
   },
   "source": [
    "$\\implies$ repeat the operation of the CNN on several images $\\implies$ LSTM = build the internal state + update the weights (with BTT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CNN Model** (*Conv2D* = interpret snapshots + *polling layers* = consolidate or abstract the interpretation) for feature extraction $\\implies$ handle just one image and turns pixels to matrix or vector.<br>\n",
    "+<br>\n",
    "The **LSTM Model** for interpreting the features across time steps.<br><br>\n",
    "![](images/CNN_LSTM_dense.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(TimeDistributed(cnn, ...))\n",
    "lstm.add(LSTM(..))\n",
    "lstm.add(Dense(...))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
